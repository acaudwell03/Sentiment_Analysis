{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5043d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from classes import *\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docstrings generated from Anysphere. (2025). Cursor [Large language model]. https://cursor.com/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a34d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- CONFIGURATION ---------\n",
    "DATASET_NAME = 'go_emotions'\n",
    "DATASET_CONFIG = 'simplified'\n",
    "PREPROCESSOR = TextPreprocessor(extra_stopwords={'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528356e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset():\n",
    "    \"\"\"Load GoEmotions dataset, remove neutral labels, and return cleaned splits.\n",
    "\n",
    "    Loads the GoEmotions dataset from HuggingFace, removes the 'neutral' label \n",
    "    from all examples, filters out examples that would have no labels after \n",
    "    neutral removal, and returns the cleaned dataset with capitalized label names.\n",
    "\n",
    "    The function processes all dataset splits (train, validation, test) and\n",
    "    ensures consistency across splits by applying the same filtering logic.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (clean_dict, clean_labels)\n",
    "            - clean_dict: DatasetDict with neutral labels removed from all splits\n",
    "            - clean_labels: List of capitalized label names excluding 'neutral'\n",
    "\n",
    "    Note:\n",
    "        This function modifies the original dataset by removing the neutral class,\n",
    "        which is common practice in emotion classification tasks where neutral\n",
    "        is often considered a baseline or uninformative class.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
    "    labels = ds[\"train\"].features[\"labels\"].feature.names\n",
    "    neutral_idx = labels.index(\"neutral\")\n",
    "\n",
    "    def remove_neutral(example):\n",
    "        example[\"labels\"] = [l for l in example[\"labels\"] if l != neutral_idx]\n",
    "        return example\n",
    "\n",
    "    def keep_if_not_empty(example):\n",
    "        return len(example[\"labels\"]) > 0\n",
    "\n",
    "    clean_dict = DatasetDict()\n",
    "    for split in ds:\n",
    "        split_ds = ds[split].map(remove_neutral)\n",
    "        split_ds = split_ds.filter(keep_if_not_empty)\n",
    "        clean_dict[split] = split_ds\n",
    "\n",
    "    clean_labels = [l.capitalize() for l in labels if l != \"neutral\"]\n",
    "    return clean_dict, clean_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cffb5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dict, clean_labels = load_and_prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "710c7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(dataset, preprocessor):\n",
    "    \"\"\"Preprocess texts using the preprocessor and filter out empty results.\n",
    "\n",
    "    Applies the text preprocessor to each text in the dataset, then filters out\n",
    "    any texts that become empty after preprocessing. This ensures that the\n",
    "    downstream models receive meaningful input data.\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace dataset containing 'text' and 'labels' fields\n",
    "        preprocessor: TextPreprocessor object with a preprocess method that\n",
    "                    takes a string and returns a processed string\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_clean, y_clean)\n",
    "            - X_clean: List of preprocessed text strings (non-empty only)\n",
    "            - y_clean: List of corresponding label lists, aligned with X_clean\n",
    "\n",
    "    Note:\n",
    "        The function maintains alignment between texts and labels, so if a text\n",
    "        is filtered out, its corresponding labels are also removed.\n",
    "    \"\"\"\n",
    "    X_clean, y_clean = [], []\n",
    "    for text, labels in zip(dataset[\"text\"], dataset[\"labels\"]):\n",
    "        processed_text = preprocessor.preprocess(text)\n",
    "        if processed_text.strip():\n",
    "            X_clean.append(processed_text)\n",
    "            y_clean.append(labels)\n",
    "    return X_clean, y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11187a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- THRESHOLD TUNING ---------\n",
    "# Code modified from OpenAI. (2025). ChatGPT [Large language model]. https://chat.openai.com/chat\n",
    "def tune_thresholds(probs, targets, low=0.1, high=0.9, steps=81):\n",
    "    \"\"\"Find optimal decision thresholds for each class to maximize F1 score.\n",
    "\n",
    "    Performs grid search over threshold values for each class to find the\n",
    "    threshold that maximizes the F1 score for that class. This is particularly\n",
    "    important for multi-label classification where the default threshold of 0.5\n",
    "    may not be optimal for all classes.\n",
    "\n",
    "    Args:\n",
    "        probs (numpy.ndarray): Predicted probabilities, shape (N, C) where\n",
    "                            N is number of samples, C is number of classes\n",
    "        targets (numpy.ndarray): Ground truth binary labels, shape (N, C)\n",
    "        low (float): Lower bound for threshold search (default: 0.1)\n",
    "        high (float): Upper bound for threshold search (default: 0.9)\n",
    "        steps (int): Number of threshold values to test between low and high\n",
    "                    (default: 81, giving ~0.01 step size)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Optimal threshold for each class, shape (C,)\n",
    "\n",
    "    Note:\n",
    "        The function uses F1 score as the optimization metric, which balances\n",
    "        precision and recall. For classes with very few positive examples,\n",
    "        this can help improve performance significantly.\n",
    "    \"\"\"\n",
    "    C = probs.shape[1]\n",
    "    best_thresholds = np.full(C, 0.5)\n",
    "    for i in range(C):\n",
    "        best_f1 = 0.0\n",
    "        for t in np.linspace(low, high, steps):\n",
    "            preds = (probs[:, i] >= t).astype(int)\n",
    "            f1 = f1_score(targets[:, i], preds, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_thresholds[i] = t\n",
    "    return best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b30dbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- MAIN PIPELINE ---------\n",
    "def main():\n",
    "    \"\"\"Execute the complete Random Forest-based emotion classification pipeline.\n",
    "\n",
    "    Orchestrates the entire machine learning pipeline including:\n",
    "    1. Dataset loading and preprocessing (removing neutral labels)\n",
    "    2. Text preprocessing using the configured TextPreprocessor\n",
    "    3. Multi-label encoding using MultiLabelBinarizer\n",
    "    4. TF-IDF feature extraction with Random Forest classification\n",
    "    5. Hyperparameter optimization using RandomizedSearchCV\n",
    "    6. Threshold tuning for optimal F1 scores\n",
    "    7. Final evaluation and metric reporting\n",
    "\n",
    "    The pipeline uses a One-vs-Rest approach with Random Forest classifiers,\n",
    "    which is well-suited for multi-label emotion classification tasks.\n",
    "\n",
    "    Outputs:\n",
    "        - Saves per-label F1 scores to 'rf_metrics.csv'\n",
    "        - Prints validation set performance metrics\n",
    "\n",
    "    Configuration:\n",
    "        Uses the global PREPROCESSOR object and DATASET_NAME/DATASET_CONFIG\n",
    "        constants defined at the module level.\n",
    "\n",
    "    Returns:\n",
    "        None: Executes the complete pipeline and saves results\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    clean_dicts, clean_labels = load_and_prepare_dataset()\n",
    "\n",
    "    # Preprocess\n",
    "    X_train, y_train = preprocess_texts(clean_dicts[\"train\"], PREPROCESSOR)\n",
    "    X_val, y_val = preprocess_texts(clean_dicts[\"validation\"], PREPROCESSOR)\n",
    "\n",
    "    # Multi-hot encoding\n",
    "    mlb = MultiLabelBinarizer(classes=list(range(len(clean_labels))))\n",
    "    y_train_bin = mlb.fit_transform(y_train)\n",
    "    y_val_bin = mlb.transform(y_val)\n",
    "\n",
    "    # Random Forest + OVR pipeline\n",
    "    base_rf = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "    pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
    "        (\"clf\", OneVsRestClassifier(base_rf))\n",
    "    ])\n",
    "\n",
    "    # Hyperparameter search\n",
    "    param_dist = {\n",
    "        \"clf__estimator__n_estimators\": [100, 200, 300],\n",
    "        \"clf__estimator__max_depth\": [10, 20, None],\n",
    "        \"clf__estimator__min_samples_split\": [2, 5],\n",
    "        \"clf__estimator__min_samples_leaf\": [1, 2],\n",
    "        \"clf__estimator__max_features\": [\"sqrt\"],\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring=\"f1_micro\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train_bin)\n",
    "    rf = search.best_estimator_\n",
    "    joblib.dump(rf, \"rf_pipeline.pkl\")\n",
    "\n",
    "    # Predict on validation\n",
    "    probs_val = rf.predict_proba(X_val)\n",
    "    if isinstance(probs_val, list):\n",
    "        probs_val = np.vstack([p[:, 1] for p in probs_val]).T\n",
    "\n",
    "    best_thresholds = tune_thresholds(probs_val, y_val_bin)\n",
    "    preds_val = (probs_val >= best_thresholds[None, :]).astype(int)\n",
    "\n",
    "    # Evaluate per-label F1\n",
    "    f1s = f1_score(y_val_bin, preds_val, average=None, zero_division=0)\n",
    "    f1_df = pd.DataFrame({\"Label\": clean_labels, \"F1-score\": f1s.round(2)})\n",
    "    print(f1_df)\n",
    "    f1_df.to_csv(\"rf_metrics.csv\", index=False)\n",
    "\n",
    "    return rf, best_thresholds, mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    rf, best_thresholds, mlb = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0711b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the trained pipeline\n",
    "rf = joblib.load(\"rf_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4792e25",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21536877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rf(text):\n",
    "    if isinstance(text, list):\n",
    "        text = text[0]\n",
    "\n",
    "    # Preprocess text with your PREPROCESSOR\n",
    "    processed_text = PREPROCESSOR.preprocess(text)\n",
    "\n",
    "    # Pipeline handles TF-IDF + RF internally\n",
    "    probs = rf.predict_proba([processed_text])\n",
    "    if isinstance(probs, list):   # OneVsRestClassifier returns a list of arrays\n",
    "        probs = np.vstack([p[:, 1] for p in probs]).T\n",
    "\n",
    "    preds = (probs >= best_thresholds[None, :]).astype(int)\n",
    "\n",
    "    pred_indices = mlb.inverse_transform(preds)[0]\n",
    "    pred_labels = [clean_labels[i] for i in pred_indices]\n",
    "\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Labels: {pred_labels}\")\n",
    "\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(str('Enter:'))\n",
    "\n",
    "predict_rf(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
